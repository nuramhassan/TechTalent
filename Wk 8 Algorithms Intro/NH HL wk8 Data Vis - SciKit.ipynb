{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 1 <br>\n",
    "\n",
    " Select one or more choices from the list of common Machine Learning Algorithms, do some investigations and write me a short summary. I am looking for the following:\n",
    "\n",
    "    • 1. Is it Supervised/Unsupervised/Reinforcement learning?\n",
    "    • 2. What does the algorithm do?\n",
    "    • 3. In which situations will it be most useful?\n",
    "    • 4. (Optional) Can you find any examples of where this algorithm has been used?\n",
    "\n",
    " • Linear Regression • Logistic Regression • Decision Tree\n",
    "• SVM (Support Vector Machine)  • Naive Bayes  • KNN (K- Nearest Neighbours)\n",
    "• K-Means • Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. The Decision Tree is part of the Classification Algorithm that falls under Supervised Learning in Machine Learning.\n",
    "# 2. Decision Tree is a tree-like graph where sorting starts from the root node to the leaf node until the target is achieved.\n",
    "# 3. Adv: - It takes consideration of each possible outcome of a decision and traces each node to the conclusion accordingly.\n",
    "#         - Decision Tree is one of the easier and reliable algorithms as it has no complex formulae or data structures.\n",
    "#         - Decision Trees assign a specific value to each problem, decision, and outcome(s). It reduces uncertainty and\n",
    "#           ambiguity and also increases clarity.\n",
    "#    Dis: - Decision trees are less appropriate for estimation and financial tasks where we need an appropriate values.\n",
    "#         - While working with continuous variables, Decision Tree is not fit as the best solution as it tends to lose\n",
    "#           information while categorizing variables.\n",
    "#4. Decision Trees could be used in emergency rooms to prioritize patient care (based on factors such as age, gender, symptoms, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
       "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    Linear Regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc. A linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. 
It is a supervised learning algorithm method, and it Is used for classification and making predictions in machine learning. The linear regression model is best used to display the relationship between variables utilizing a sloped straight line.
Mathematically, we can represent a linear regression as:
y= a0+a1x+ ε
Here,
Y= Dependent Variable (Target Variable)
X= Independent Variable (predictor Variable)
a0= intercept of the line (Gives an additional degree of freedom)
a1 = Linear regression coefficient (scale factor to each input value).
ε = random error
The values for x and y variables are training datasets for Linear Regression model representation.
Types of Linear Regression
Linear regression can be further divided into two types of the algorithm:
o	Simple Linear Regression:
If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.
o	Multiple Linear regression:
If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression.
Linear Regression Line
A linear line showing the relationship between the dependent and independent variables is called a regression line. A regression line can show two types of relationships:
o	Positive Linear Relationship:
If the dependent variable increases on the Y-axis and the independent variable increases on X-axis, then such a relationship is termed as a positive linear relationship.
 
o	Negative Linear Relationship:
If the dependent variable decreases on the Y-axis and the independent variable increases on the X-axis, then such a relationship is called a negative linear relationship.
 
Finding the best fit line:
When working with linear regression, our main goal is to find the best fit line which means the error between predicted values and actual values should be minimized. The best fit line will have the least error.
The different values for weights or the coefficient of lines (a0, a1) give a different line of regression, so we need to calculate the best values for a0 and a1 to find the best fit line, so to calculate this we use the cost function.
For Linear Regression, we use the Mean Squared Error (MSE) cost function, which is the average of squared error that occurred between the predicted values and actual values.
   "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2\n",
    " From this website select follow and complete the Linear Regression Model.\n",
    "https://stackabuse.com/linear-regression-in-python-with-scikit-learn/\n",
    "   Graph is on Word doc\n",
    "\n",
    "    # In the above we performed linear regression involving two variables. Almost all real world problems\n",
    "    # that you are going to encounter will have more than two variables. Linear regression involving multiple variables\n",
    "    # is called \"multiple linear regression\". The steps to perform multiple linear regression are almost similar to\n",
    "    # that of simple linear regression. The difference lies in the evaluation. You can use it to find out which factor\n",
    "    # has the highest impact on the predicted output and how different variables relate to each other.\n",
    "\n",
    "    # https://stackabuse.com/linear-regression-in-python-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    dataset = pd.read_csv('D:\\Documents\\Tech Talent Academy Bootcamp\\student_scores.csv')\n",
    "    # View all data.\n",
    "    print(\"My imported Data set looks like this: \\n\", dataset)\n",
    "    # Find how many rows and columns there are.\n",
    "    print(\"My dataset has these rows and columns: \", dataset.shape)\n",
    "    # Using head() to print out the top 5 pieces of data.\n",
    "    print(\"The top 5 pieces of data: \\n\", dataset.head())\n",
    "    # See the statistical details of the dataset using describe().\n",
    "    print(\"The statistical details: \\n\", dataset.describe())\n",
    "\n",
    "    # Plot Data\n",
    "    dataset.plot(x='Hours', y='Scores', style='o')\n",
    "    plt.title('Hours vs Percentage')\n",
    "    plt.xlabel('Hours Studied')\n",
    "    plt.ylabel('Percentage Score')\n",
    "    plt.show()\n",
    "\n",
    "    # Divide the data into \"attributes\" and \"labels\".\n",
    "    # Attributes are the independent variables while labels are dependent variables whose values are to be predicted.\n",
    "\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, 1].values\n",
    "\n",
    "    #  Split this data into training and test sets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    # Splits 80% of the data to training set while 20% of the data to test set. The test_size variable is where we\n",
    "    # actually specify the proportion of test set.\n",
    "\n",
    "    # Time to train our algorithm.\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # To retrieve the intercept:\n",
    "    print(\"The value of the intercept and slop calculated by the linear regression algorithm for our dataset: \", regressor.intercept_)\n",
    "\n",
    "    # Retrieving the slope (coefficient of x):\n",
    "    print(\"Retrieving the slope: \", regressor.coef_)\n",
    "\n",
    "    #  Predictions on the test data.\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    #  Compare the actual output values for X_test with the predicted values.\n",
    "    df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "    print(\"Compare actual output with predicted values: \\n\", df)\n",
    "\n",
    "    # Evaluate the performance of algorithm.\n",
    "    # For regression algorithms, three evaluation metrics are commonly used:\n",
    "    # Mean Absolute Error (MAE) is the mean of the absolute value of the errors. It is calculated as: MAE = (1/n) * Σ|yi – xi|\n",
    "    # Mean Squared Error (MSE) is the mean of the squared errors and is calculated as: MSE = Σ (P i – O i) 2 / n\n",
    "    # Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors: RMSE = √Σ (Pi – Oi)2 / n\n",
    "    # The Scikit-Learn library comes with pre-built functions that can be used to find out these values for us.\n",
    "\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the above we performed linear regression involving two variables. Almost all real world problems\n",
    "That you are going to encounter will have more than two variables. Linear regression involving multiple variables\n",
    "Is called \"multiple linear regression\". The steps to perform multiple linear regression are almost similar to\n",
    "That of simple linear regression. The difference lies in the evaluation. You can use it to find out which factor\n",
    "Has the highest impact on the predicted output and how different variables relate to each other.\n",
    " https://stackabuse.com/linear-regression-in-python-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    # -------------------------TERMINAL OUTPUT----------------------------#\n",
    "\n",
    "    # My imported Data set looks like this:\n",
    "    #      Hours  Scores\n",
    "    # 0     2.5      21\n",
    "    # 1     5.1      47\n",
    "    # 2     3.2      27\n",
    "    # 3     8.5      75\n",
    "    # 4     3.5      30\n",
    "    # 5     1.5      20\n",
    "    # 6     9.2      88\n",
    "    # 7     5.5      60\n",
    "    # 8     8.3      81\n",
    "    # 9     2.7      25\n",
    "    # 10    7.7      85\n",
    "    # 11    5.9      62\n",
    "    # 12    4.5      41\n",
    "    # 13    3.3      42\n",
    "    # 14    1.1      17\n",
    "    # 15    8.9      95\n",
    "    # 16    2.5      30\n",
    "    # 17    1.9      24\n",
    "    # 18    6.1      67\n",
    "    # 19    7.4      69\n",
    "    # 20    2.7      30\n",
    "    # 21    4.8      54\n",
    "    # 22    3.8      35\n",
    "    # 23    6.9      76\n",
    "    # 24    7.8      86\n",
    "    # My dataset has these rows and columns:  (25, 2)\n",
    "    # The top 5 pieces of data:\n",
    "    #     Hours  Scores\n",
    "    # 0    2.5      21\n",
    "    # 1    5.1      47\n",
    "    # 2    3.2      27\n",
    "    # 3    8.5      75\n",
    "    # 4    3.5      30\n",
    "    # The statistical details:\n",
    "    #             Hours     Scores\n",
    "    # count  25.000000  25.000000\n",
    "    # mean    5.012000  51.480000\n",
    "    # std     2.525094  25.286887\n",
    "    # min     1.100000  17.000000\n",
    "    # 25%     2.700000  30.000000\n",
    "    # 50%     4.800000  47.000000\n",
    "    # 75%     7.400000  75.000000\n",
    "    # max     9.200000  95.000000\n",
    "    #The value of the intercept and slop calculated by the linear regression algorithm for our dataset:  2.0181600414346974\n",
    "    # Retrieving the slope:  [9.91065648]\n",
    "    # Compare actual output with predicted values:\n",
    "    #     Actual  Predicted\n",
    "    # 0      20  16.884145\n",
    "    # 1      27  33.732261\n",
    "    # 2      69  75.357018\n",
    "    # 3      30  26.794801\n",
    "    # 4      62  60.491033\n",
    "    #Mean Absolute Error: 4.183859899002975\n",
    "    #Mean Squared Error: 21.598769307217406\n",
    "    #Root Mean Squared Error: 4.647447612100367"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "399bb1343ed0b6c28f417d1fb9d9cb0b4717543d8e83fb2aafba1121d5df8252"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
